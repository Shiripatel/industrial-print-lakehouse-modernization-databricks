# Global Industrial Print â€“ Data Platform Modernization with Databricks SQL

This repository is a **consulting-style case study** showing how a global industrial print client (name anonymized) modernized its data platform using the **Databricks Lakehouse** on AWS.

It is designed as a **portfolio project** to demonstrate skills in:
- Data architecture & medallion design (Bronze / Silver / Gold)
- Databricks SQL and Delta Lake
- Unity Catalog governance
- Secure data sharing with Delta Sharing
- Building data products for internal and external customers

> All data in this repository is dummy/simulated and safe to publish.

---

## ðŸ— High-Level Architecture

1. **Source systems**
   - Print workflow portal and operational databases (simulated via JSON files)
   - Events such as job scans, status updates and machine telemetry

2. **Bronze layer (raw)**
   - Raw ingested JSON from the operational systems stored as Delta tables

3. **Silver layer (curated)**
   - Cleaned and normalized data, conformed dimensions, basic data quality rules

4. **Gold layer (serving)**
   - Star-schema style marts for internal BI and external customer insights

5. **Governance & Sharing**
   - Unity Catalog for catalogs / schemas / tables / permissions
   - Delta Sharing for secure access to curated datasets for partners and customers

See `architecture/lakehouse-architecture.png` for a visual representation.

---

## ðŸ“ Repository Structure

```text
industrial-print-lakehouse-modernization-databricks/
â”œâ”€ README.md
â”œâ”€ docs/
â”‚   â”œâ”€ 01-business-context.md
â”‚   â”œâ”€ 02-legacy-architecture.md
â”‚   â”œâ”€ 03-target-architecture.md
â”‚   â”œâ”€ 04-security-governance.md
â”‚   â””â”€ 05-business-impact.md
â”œâ”€ architecture/
â”‚   â”œâ”€ lakehouse-architecture.png
â”‚   â””â”€ lakehouse-architecture.txt
â”œâ”€ notebooks/
â”‚   â”œâ”€ 01_bronze_ingestion.ipynb
â”‚   â”œâ”€ 02_silver_transformations.ipynb
â”‚   â”œâ”€ 03_gold_dwh_modeling.ipynb
â”‚   â””â”€ 04_customer_insights_gold.ipynb
â”œâ”€ sql/
â”‚   â”œâ”€ create_bronze_tables.sql
â”‚   â”œâ”€ create_silver_views.sql
â”‚   â”œâ”€ create_gold_marts.sql
â”‚   â””â”€ dashboards_queries.sql
â”œâ”€ infra/
â”‚   â”œâ”€ databricks_job_definitions.json
â”‚   â””â”€ sample_unity_catalog_policies.md
â””â”€ samples/
    â”œâ”€ sample_orders.json
    â”œâ”€ sample_providers.json
    â””â”€ sample_events.json
```

---

## ðŸš€ Getting Started (Databricks)

1. Upload the repository (or selected folders) to your Databricks workspace or mount it via a repo.
2. Create a cluster with the **Spark** runtime of your choice.
3. Create these catalogs/schemas or adapt the SQL:
   - `industrial_print.bronze`
   - `industrial_print.silver`
   - `industrial_print.gold`
4. Run notebooks in order:
   1. `01_bronze_ingestion`
   2. `02_silver_transformations`
   3. `03_gold_dwh_modeling`
   4. `04_customer_insights_gold`

---

## ðŸ”§ Tech Stack

- **Cloud**: AWS (simulated)
- **Platform**: Databricks Data Intelligence Platform
- **Storage**: Delta Lake on S3 (simulated paths: `/mnt/bronze`, `/mnt/silver`, `/mnt/gold`)
- **Governance**: Unity Catalog (policies described in `infra/sample_unity_catalog_policies.md`)
- **Consumption**: Databricks SQL, BI tools via SQL Warehouse, Delta Sharing

---

## âœ¨ Portfolio Usage

You can reference this project on your resume / LinkedIn as:

> "Designed and implemented a Databricks Lakehouse architecture for a global industrial print client (anonymized), including medallion data modeling, Delta Lake pipelines, Unity Catalog governance and Delta Sharing to external partners."

Feel free to fork and extend with your own dashboards, ML notebooks or infra-as-code.
